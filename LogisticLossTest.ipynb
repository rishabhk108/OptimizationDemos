{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Data\n",
    "\n",
    "We first generate a random dataset with number of features (m = 10) and number of instances (n = 100)\n",
    "We also generate a random label vector y \\in {-1,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 -1 -1 -1  1 -1  1  1  1 -1  1 -1  1 -1  1  1  1 -1 -1  1  1 -1 -1\n",
      " -1 -1 -1 -1  1 -1 -1  1 -1 -1  1  1  1 -1  1 -1 -1 -1  1  1  1 -1  1  1\n",
      "  1 -1  1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1  1  1 -1  1  1 -1 -1  1  1 -1\n",
      "  1  1  1 -1  1  1  1 -1 -1  1  1 -1 -1  1  1 -1 -1 -1 -1  1 -1 -1 -1  1\n",
      "  1 -1 -1 -1]\n",
      "[[0.15081579 0.02060775 0.76881967 0.72560174 0.46650267 0.98982589\n",
      "  0.91384915 0.81710788 0.29143393 0.55795121]\n",
      " [0.86394994 0.90318432 0.78147468 0.81914016 0.1059121  0.44348636\n",
      "  0.74783135 0.17751439 0.95526117 0.60228485]\n",
      " [0.68263416 0.28028993 0.73856586 0.96458103 0.48799289 0.31165456\n",
      "  0.08107802 0.89068509 0.36414014 0.01643942]\n",
      " [0.49802953 0.38370109 0.82619215 0.98252769 0.35166422 0.46767241\n",
      "  0.34375141 0.11204718 0.57158609 0.63917095]\n",
      " [0.18790744 0.54965513 0.99397204 0.20613624 0.57725534 0.73939673\n",
      "  0.16331439 0.65325905 0.34976847 0.4567399 ]\n",
      " [0.61553026 0.61245476 0.40400442 0.98580638 0.68637067 0.80192352\n",
      "  0.7057647  0.96929697 0.45785726 0.05656022]\n",
      " [0.15378803 0.88738392 0.56885074 0.37457509 0.63189775 0.91899163\n",
      "  0.94779187 0.0703395  0.2099531  0.50982056]\n",
      " [0.49422764 0.95925808 0.86412938 0.36245742 0.14789465 0.69404695\n",
      "  0.74122011 0.54189698 0.18722263 0.50705846]\n",
      " [0.49042015 0.40316866 0.76260622 0.34200398 0.48631196 0.4694385\n",
      "  0.2602999  0.88573128 0.10700275 0.18256179]\n",
      " [0.90415737 0.81666418 0.8540644  0.95876546 0.33759363 0.53350651\n",
      "  0.60018472 0.02201842 0.21450841 0.20511974]\n",
      " [0.87543599 0.09669685 0.45376799 0.75592491 0.25840655 0.53541738\n",
      "  0.11306012 0.27945036 0.56671048 0.68377008]\n",
      " [0.9180633  0.66882064 0.3406679  0.50124086 0.47235231 0.61494773\n",
      "  0.35199386 0.98686249 0.18485418 0.73515661]\n",
      " [0.15062198 0.92140241 0.80701602 0.81049142 0.00115152 0.72176245\n",
      "  0.8411878  0.22283606 0.9609365  0.83310077]\n",
      " [0.74772602 0.85420895 0.90721657 0.53653877 0.90049061 0.31412872\n",
      "  0.30477273 0.79008855 0.87818515 0.74609516]\n",
      " [0.4672885  0.68839039 0.83136809 0.25340632 0.75576629 0.98211634\n",
      "  0.57240517 0.33442485 0.36690828 0.09833282]\n",
      " [0.15809041 0.71236439 0.38040214 0.71037227 0.21614151 0.863222\n",
      "  0.01161398 0.56182743 0.61788184 0.73949381]\n",
      " [0.07535999 0.3291597  0.02665491 0.7350559  0.8118688  0.48510404\n",
      "  0.66934082 0.79521452 0.86534658 0.62536516]\n",
      " [0.5696147  0.34409937 0.70515236 0.69744395 0.98414841 0.06508207\n",
      "  0.97233159 0.14439528 0.98135178 0.50612845]\n",
      " [0.03389694 0.42003621 0.6006504  0.82574441 0.87668729 0.09146244\n",
      "  0.82510403 0.31747265 0.87242905 0.86736503]\n",
      " [0.12842019 0.20837674 0.7774347  0.9148231  0.36521668 0.18638835\n",
      "  0.83628288 0.74894916 0.89361158 0.69087121]\n",
      " [0.53366886 0.35109523 0.11203665 0.28684488 0.17329334 0.73815186\n",
      "  0.21001316 0.91701312 0.34136749 0.53893868]\n",
      " [0.23815314 0.14926395 0.94751564 0.43055888 0.06652178 0.07255165\n",
      "  0.15448903 0.58142142 0.065054   0.79929069]\n",
      " [0.14856593 0.10072318 0.95227488 0.25417441 0.1448282  0.85369206\n",
      "  0.49852392 0.90281562 0.30525798 0.15824039]\n",
      " [0.27422092 0.95445014 0.96920162 0.39296414 0.59668096 0.70311756\n",
      "  0.12110895 0.17886279 0.97332931 0.79850841]\n",
      " [0.9038036  0.93717946 0.20839064 0.88387182 0.94141587 0.60518385\n",
      "  0.49947413 0.05809995 0.61564694 0.66572321]\n",
      " [0.48885243 0.33612467 0.88892477 0.51824355 0.34232841 0.9185741\n",
      "  0.26140817 0.697149   0.00671581 0.74222663]\n",
      " [0.22929496 0.41802644 0.10973218 0.61560231 0.42246689 0.23694141\n",
      "  0.27376503 0.86636963 0.12088821 0.14420129]\n",
      " [0.44091856 0.43595142 0.51386613 0.61078379 0.12243992 0.62097763\n",
      "  0.72674161 0.79790423 0.60872336 0.98029798]\n",
      " [0.29175415 0.79761027 0.72897983 0.27684871 0.06402785 0.98239855\n",
      "  0.42964625 0.53499877 0.7346432  0.81249562]\n",
      " [0.99762639 0.54703507 0.26469741 0.6648704  0.66448966 0.33835094\n",
      "  0.42589995 0.86022828 0.09003161 0.45149924]\n",
      " [0.68775466 0.36544723 0.99066554 0.612593   0.82442568 0.42981277\n",
      "  0.82997508 0.43772405 0.41684529 0.50404119]\n",
      " [0.09694504 0.55957835 0.79325614 0.59066279 0.4589773  0.35376927\n",
      "  0.75515759 0.35761892 0.28148166 0.60876288]\n",
      " [0.76620517 0.50641326 0.63212151 0.49472254 0.82812568 0.60584475\n",
      "  0.38841267 0.56282148 0.97855796 0.42203269]\n",
      " [0.9565439  0.97806428 0.51013511 0.47953926 0.77274803 0.95516173\n",
      "  0.96550984 0.30571265 0.7534045  0.89085851]\n",
      " [0.8564249  0.98140248 0.58972607 0.42305435 0.40893588 0.99584674\n",
      "  0.52505101 0.36031057 0.17462077 0.97383493]\n",
      " [0.31532905 0.80575245 0.52754843 0.97916433 0.6566001  0.91767103\n",
      "  0.80142807 0.63801812 0.13654839 0.49656669]\n",
      " [0.05241012 0.43063631 0.85208686 0.61750084 0.8883643  0.6082281\n",
      "  0.63228284 0.99971805 0.31528238 0.11248778]\n",
      " [0.3516571  0.86414141 0.88732963 0.12788129 0.89660523 0.31340897\n",
      "  0.9413197  0.34026879 0.12840475 0.06304223]\n",
      " [0.58766523 0.07334131 0.97527575 0.34464359 0.49039575 0.48225383\n",
      "  0.1849598  0.75227376 0.36126531 0.62224039]\n",
      " [0.03266532 0.00911    0.08600772 0.25240737 0.63340264 0.25988349\n",
      "  0.22705018 0.09880909 0.94849126 0.7236524 ]\n",
      " [0.38596105 0.82192061 0.65875965 0.61824955 0.93582635 0.56216887\n",
      "  0.12679865 0.68972849 0.18114718 0.48202791]\n",
      " [0.53664303 0.75775355 0.3079981  0.42542869 0.84247211 0.66357127\n",
      "  0.52595421 0.65648777 0.08626743 0.15465527]\n",
      " [0.39620231 0.92506723 0.73392596 0.68747504 0.89188813 0.83926433\n",
      "  0.14180986 0.72137587 0.16302076 0.65328961]\n",
      " [0.10979038 0.26026927 0.99593821 0.54758466 0.44429088 0.6481904\n",
      "  0.31818729 0.34156364 0.17740772 0.80008603]\n",
      " [0.98614614 0.28259321 0.27757007 0.36475997 0.90972793 0.39054735\n",
      "  0.26900204 0.39676712 0.57265151 0.21115958]\n",
      " [0.72184968 0.97381811 0.88600505 0.23073937 0.11833531 0.09477845\n",
      "  0.84434557 0.28882503 0.74845584 0.21635722]\n",
      " [0.28984432 0.76384092 0.33881152 0.73067983 0.40677298 0.65022436\n",
      "  0.66625292 0.00682215 0.16694174 0.52961786]\n",
      " [0.99493372 0.34787581 0.15484845 0.53343817 0.25643502 0.39519103\n",
      "  0.86128651 0.74738043 0.19554773 0.36384356]\n",
      " [0.24256352 0.63456489 0.59673422 0.63727451 0.28752104 0.41077445\n",
      "  0.18182157 0.16556496 0.54487561 0.46790051]\n",
      " [0.18383869 0.32203149 0.57841781 0.9574756  0.75478043 0.81608851\n",
      "  0.1566852  0.29107981 0.08866971 0.60854146]\n",
      " [0.81080965 0.10942493 0.56086568 0.58000453 0.65429026 0.75882658\n",
      "  0.68794718 0.39055171 0.99837691 0.42474002]\n",
      " [0.96057278 0.02631284 0.53792768 0.20636392 0.54649082 0.23407777\n",
      "  0.5714685  0.32063855 0.27114919 0.31033868]\n",
      " [0.33229826 0.12076475 0.01815149 0.12172731 0.49226861 0.14637234\n",
      "  0.47111647 0.08516909 0.7980051  0.22890693]\n",
      " [0.94532076 0.74540643 0.24788903 0.20687326 0.54168595 0.33782576\n",
      "  0.18245814 0.13947442 0.73821552 0.73481445]\n",
      " [0.38867268 0.66887301 0.30347465 0.41258746 0.4694149  0.03854983\n",
      "  0.34630716 0.08505476 0.42185288 0.75459772]\n",
      " [0.49075835 0.01964409 0.79279822 0.96051514 0.28435455 0.10329458\n",
      "  0.65072794 0.3814806  0.57992734 0.60055211]\n",
      " [0.1851037  0.69305733 0.13320545 0.8449315  0.46676122 0.29323117\n",
      "  0.60368791 0.48259803 0.89970219 0.7681392 ]\n",
      " [0.37372667 0.88618242 0.73580166 0.37697396 0.11711645 0.48556035\n",
      "  0.63945846 0.68795012 0.46154467 0.77908143]\n",
      " [0.76750642 0.00930551 0.89823227 0.95398014 0.9107683  0.27980534\n",
      "  0.17135234 0.72854585 0.27200817 0.65420378]\n",
      " [0.28904174 0.9031137  0.39589914 0.84730449 0.93511883 0.8839768\n",
      "  0.97481342 0.81821533 0.8825361  0.93910619]\n",
      " [0.59735696 0.44153898 0.7190902  0.7008601  0.88561751 0.50659644\n",
      "  0.50525217 0.38816282 0.87174965 0.32638283]\n",
      " [0.93044485 0.46037723 0.49402849 0.31524041 0.03100103 0.21258426\n",
      "  0.15143389 0.010853   0.2902692  0.64278252]\n",
      " [0.29923784 0.0448027  0.51333748 0.89929923 0.88408837 0.91606295\n",
      "  0.35729563 0.39626187 0.2379128  0.64818096]\n",
      " [0.06207415 0.11019885 0.98887958 0.87624589 0.321871   0.56445675\n",
      "  0.03181735 0.48102748 0.0546243  0.65131603]\n",
      " [0.7381504  0.33533281 0.19630374 0.50274241 0.80636755 0.59245469\n",
      "  0.04274858 0.90741287 0.57248406 0.53527741]\n",
      " [0.82799448 0.78070914 0.02821645 0.28230751 0.4474667  0.10874\n",
      "  0.87756892 0.72785564 0.37689711 0.66837769]\n",
      " [0.53783132 0.22086741 0.31933338 0.42460388 0.99333961 0.67425711\n",
      "  0.91120082 0.4531589  0.82659041 0.15887741]\n",
      " [0.02256037 0.97911003 0.25935157 0.02226363 0.88662738 0.37051827\n",
      "  0.22187647 0.01725751 0.96992986 0.71551153]\n",
      " [0.35942887 0.06095493 0.33485194 0.72391839 0.33296222 0.46839733\n",
      "  0.29581757 0.33338534 0.4481541  0.14454273]\n",
      " [0.57402352 0.88587164 0.17247552 0.43013008 0.68498632 0.70756068\n",
      "  0.77421288 0.02710013 0.26096935 0.19799809]\n",
      " [0.32110049 0.82177609 0.72167422 0.54103508 0.99910456 0.13642862\n",
      "  0.15602738 0.09673836 0.95033286 0.41033086]\n",
      " [0.46861752 0.73434432 0.14466662 0.07144511 0.71992345 0.25887271\n",
      "  0.28929082 0.70623927 0.71071436 0.00323322]\n",
      " [0.13327557 0.9081652  0.30079265 0.9185565  0.67922266 0.90396095\n",
      "  0.44227169 0.76385528 0.32704301 0.19985448]\n",
      " [0.39399036 0.91897027 0.10552592 0.73677225 0.95234161 0.84559137\n",
      "  0.22243543 0.63171405 0.9545032  0.2470448 ]\n",
      " [0.74559273 0.13285235 0.09087807 0.01441144 0.45178314 0.46161244\n",
      "  0.28154838 0.1578316  0.60282596 0.34179827]\n",
      " [0.83757455 0.28511733 0.41423408 0.85564533 0.87562223 0.66890484\n",
      "  0.54135365 0.84476683 0.73811782 0.25935492]\n",
      " [0.89013008 0.1077483  0.27736242 0.35453589 0.33665939 0.29722096\n",
      "  0.12509471 0.83656328 0.12469217 0.35835712]\n",
      " [0.19500348 0.59787844 0.51824233 0.87959082 0.93557225 0.71772766\n",
      "  0.10654139 0.16682067 0.35441476 0.73874453]\n",
      " [0.24800677 0.08928022 0.27929397 0.37469936 0.67961741 0.46510035\n",
      "  0.18719554 0.55721621 0.89745099 0.30844033]\n",
      " [0.44801088 0.28863763 0.55476367 0.92340172 0.06432689 0.16546185\n",
      "  0.77906672 0.9945446  0.21871902 0.14143689]\n",
      " [0.67978591 0.20732342 0.75136882 0.04417352 0.46554266 0.5837054\n",
      "  0.64001526 0.14999122 0.01318914 0.71283091]\n",
      " [0.29847867 0.83040856 0.7803665  0.8348507  0.32972064 0.6406533\n",
      "  0.30607461 0.9964972  0.5044127  0.97817459]\n",
      " [0.35680942 0.84404825 0.51509013 0.10370779 0.83765133 0.91008325\n",
      "  0.03542499 0.64370778 0.28976983 0.76411447]\n",
      " [0.94865501 0.54694666 0.01836578 0.90248535 0.96646505 0.33310367\n",
      "  0.13414138 0.27208923 0.80180008 0.54991031]\n",
      " [0.30429841 0.76333426 0.01485642 0.12887294 0.98431904 0.53296599\n",
      "  0.17368955 0.95112522 0.68062443 0.31217883]\n",
      " [0.27497356 0.90416082 0.10232113 0.55252881 0.91237104 0.40258708\n",
      "  0.73097085 0.24531319 0.11626053 0.46886287]\n",
      " [0.31085124 0.20101009 0.61765216 0.69831077 0.19009771 0.17496637\n",
      "  0.73029228 0.74905952 0.63611587 0.54213331]\n",
      " [0.97853099 0.09315488 0.19628694 0.83877845 0.89591182 0.17932265\n",
      "  0.73363528 0.01204228 0.99266682 0.09446083]\n",
      " [0.53401515 0.07717432 0.22220255 0.8550433  0.75125644 0.45540716\n",
      "  0.1528508  0.59100156 0.1386998  0.43933924]\n",
      " [0.67563467 0.92440724 0.58916281 0.06602604 0.38125242 0.20692042\n",
      "  0.28491161 0.90540683 0.90127137 0.36089936]\n",
      " [0.65975041 0.1551392  0.43828457 0.32021911 0.86548361 0.58647614\n",
      "  0.50944361 0.743662   0.75048125 0.38924531]\n",
      " [0.42311061 0.11841456 0.00392209 0.4854539  0.39777286 0.6857571\n",
      "  0.58300033 0.18205163 0.41969084 0.7415865 ]\n",
      " [0.49009937 0.0837165  0.09114982 0.68634383 0.78962074 0.01117273\n",
      "  0.24463515 0.71897868 0.95139674 0.22583883]\n",
      " [0.04329357 0.88331014 0.33355444 0.7597401  0.69667437 0.86553551\n",
      "  0.07565831 0.66924024 0.16302389 0.72846505]\n",
      " [0.04136035 0.13705953 0.73973672 0.90780428 0.94694561 0.65048365\n",
      "  0.23432905 0.41357856 0.060315   0.00539698]\n",
      " [0.0810388  0.64566237 0.30763682 0.14034469 0.85647749 0.51785877\n",
      "  0.4791941  0.38914424 0.74298705 0.61855507]\n",
      " [0.05600949 0.85828769 0.57800569 0.75586877 0.23074984 0.43463464\n",
      "  0.59481337 0.30910942 0.43623723 0.65721967]\n",
      " [0.51546502 0.80471988 0.44852733 0.5531339  0.90974801 0.87400139\n",
      "  0.98851505 0.54747228 0.95295806 0.91964384]\n",
      " [0.30357634 0.57161643 0.46857144 0.98923952 0.96909309 0.42412481\n",
      "  0.50985028 0.21782108 0.07341004 0.3426428 ]\n",
      " [0.94980361 0.39952113 0.70864687 0.79477718 0.47542038 0.23614131\n",
      "  0.08668472 0.73100656 0.61389363 0.36876559]]\n"
     ]
    }
   ],
   "source": [
    "n = 100 # Number of instances\n",
    "m = 10  # Number of Features \n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple naive Implementation of the Logistic Loss \n",
    "\n",
    "Below is a simple naive implementation of Logistic Loss. We directly plug in the formula with a simple for loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    f = 0\n",
    "    g = 0\n",
    "    for i in range(len(X)):\n",
    "        featureweightProd = np.dot(X[i],w)\n",
    "        f = f + np.log(1 + np.exp(-y[i]*featureweightProd))\n",
    "        g = g + -y[i]/(1 + np.exp(y[i]*featureweightProd))*X[i]\n",
    "    f = f + 0.5*lam*np.sum(w*w)\n",
    "    g = g + lam*w.reshape(1,-1)\n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.0070149898529052734\n",
      "Function value = [134.67014975]\n",
      "Printing Gradient:\n",
      "[[21.96211495 21.00490527 23.2016018  25.46196684 26.97171059 20.9702425\n",
      "  19.03946367 20.76246686 22.74056883 21.53568644]]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = LogisticLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great! Can we ship this code? Well, we might be able to, but we should ideally test this out more! \n",
    "\n",
    "For one, the above scenario is very simplistic! In practice we have much larger number of features and instances. Let us increase the number of features m to 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.010155200958251953\n",
      "Function value = [inf]\n",
      "Printing Gradient:\n",
      "[[28.04187539 28.94383048 29.70031591 ... 31.13802037 27.57701899\n",
      "  25.72323391]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10000\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LogisticLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving numerical issues with log-sum-exp!\n",
    "\n",
    "We see that we have run into numerical issues! The main reason is the naive implementation does not have numerical stability in the definition of log-sum-exp. Once -y*x*w becomes large positive, exp(-y*x*w) will become Inf and Log(Inf) is Inf!\n",
    "\n",
    "We can solve this by either defining our own function which protects against such numerical issues, or use the inbuilt function logaddexp. \n",
    "\n",
    "However it is super critical to be aware of numerical issues!\n",
    "\n",
    "Lets define a function LogisticLossFor which is the same as above just fixing the numerical issue above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossFor(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    m = X.shape[0]\n",
    "    f = 0\n",
    "    g = 0\n",
    "    for i in range(len(X)):\n",
    "        featureweightProd = np.dot(X[i],w)\n",
    "        f = f + np.logaddexp(0, -y[i]*featureweightProd)\n",
    "        g = g + -y[i]/(1 + np.exp(y[i]*featureweightProd))*X[i]\n",
    "    f = f + 0.5*lam*np.sum(w*w)\n",
    "    g = g + lam*w.reshape(1,-1)\n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.00616002082824707\n",
      "Function value = [144628.44285299]\n",
      "Printing Gradient:\n",
      "[[28.04187539 28.94383048 29.70031591 ... 31.13802037 27.57701899\n",
      "  25.72323391]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = LogisticLossFor(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets make sure the two definitions are the same!\n",
    "\n",
    "The above clearly fixed the Inf issue! However every time we write a new code, we should ensure our code is correct and the best way to do it is by checking with a previous working version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.0017960071563720703\n",
      "Function value Naive = [146.9464845]\n",
      "Printing Gradient Naive:\n",
      "[[22.21932155 21.94511061 25.01754849 23.57480241 24.7308955  26.42184361\n",
      "  22.82138964 23.90039954 23.95644553 24.0426967 ]]\n",
      "Time Taken = 0.002031087875366211\n",
      "Function value For = [146.9464845]\n",
      "Printing Gradient For:\n",
      "[[22.21932155 21.94511061 25.01754849 23.57480241 24.7308955  26.42184361\n",
      "  22.82138964 23.90039954 23.95644553 24.0426967 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "m = 10\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f1,g1] = LogisticLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value Naive = \" + str(f1))\n",
    "print(\"Printing Gradient Naive:\")\n",
    "print(g1)\n",
    "\n",
    "start = time.time()\n",
    "[f2,g2] = LogisticLossFor(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value For = \" + str(f2))\n",
    "print(\"Printing Gradient For:\")\n",
    "print(g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Loop in Python == Slow Code\n",
    "\n",
    "Great, we have fixed the Inf issue now! But while this code might be correct, is this going to be fast? We have a For loop in python which is clearly an issue!\n",
    "\n",
    "First let us see how slow the code is! Let us increase n to 1000000 and m to 1000, which are somewhat more realistic (though still far from real world)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 13.130612850189209\n",
      "Function value = [12994789.70183541]\n"
     ]
    }
   ],
   "source": [
    "n = 1000000\n",
    "m = 100\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LogisticLossFor(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the code!\n",
    "\n",
    "With m = 100, it takes around 10 seconds and with m = 1000, it is already 65 seconds (you can try it at home). \n",
    "With each 10x increase in m or n, the time taken increases exponentially!\n",
    "\n",
    "Lets now vectorize the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLoss(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    m = X.shape[0]\n",
    "    Xw = np.matmul(X,w)\n",
    "    yT = y.reshape(-1,1)\n",
    "    yXw = np.multiply(yT,Xw)\n",
    "    f = np.sum(np.logaddexp(0,-yXw)) + 0.5*lam*np.sum(w*w)\n",
    "    gMul = np.exp(-yXw)/(1 + np.exp(-yXw))\n",
    "    ymul = -1*yT*gMul\n",
    "    g =  np.matmul(ymul.reshape(1,-1),X) + lam*w.reshape(1,-1)\n",
    "    #g = np.dot(X.T,ymul) + lam*w.reshape(1,-1)\n",
    "    g = g.reshape(-1,1)\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value = 133.24380504169818\n",
      "Function value = [133.24380504]\n"
     ]
    }
   ],
   "source": [
    "n = 100 # Number of instances\n",
    "m = 10  # Number of Features \n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "[f1,g1] = LogisticLoss(w,X,y,1)\n",
    "print(\"Function value = \" + str(f1))\n",
    "\n",
    "[f2,g2] = LogisticLossFor(w,X,y,1)\n",
    "print(\"Function value = \" + str(f2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking gradient implementations!\n",
    "\n",
    "So far so good! But how do we verify if our gradient implementation is correct?\n",
    "We can test out our loss function analytically, but what if we make a mistake in computing the gradient? We can numerically compute the gradient to ensure it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossFun(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    m = X.shape[0]\n",
    "    Xw = X.dot(w)\n",
    "    yT = y.reshape(-1,1)\n",
    "    yXw = np.multiply(yT,Xw)    \n",
    "    f = np.sum(np.logaddexp(0,-yXw)) + 0.5*lam*np.sum(w*w)\n",
    "    return f\n",
    "\n",
    "\n",
    "def numericalGrad(funObj, w,epsilon):\n",
    "    m = len(w)\n",
    "    grad = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        wp = np.copy(w)\n",
    "        wn = np.copy(w)\n",
    "        wp[i] = w[i] + epsilon\n",
    "        wn[i] = w[i] - epsilon\n",
    "        grad[i] = (funObj(wp) - funObj(wn))/(2*epsilon)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.24355436930067\n",
      "123.24355436930067\n",
      "[20.91184115 21.99328719 21.94660453 18.89297607 19.07018543 21.4787832\n",
      " 18.11095274 20.62350291 20.89137752 19.96859567]\n",
      "[[20.91181858]\n",
      " [21.99332411]\n",
      " [21.94641886]\n",
      " [18.89291572]\n",
      " [19.07008602]\n",
      " [21.47886715]\n",
      " [18.1108865 ]\n",
      " [20.62333919]\n",
      " [20.89130566]\n",
      " [19.96848919]]\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "funObj = lambda w: LogisticLossFun(w,X,y,1)\n",
    "[f,g] = LogisticLoss(w,X,y,1)\n",
    "gn = numericalGrad(funObj, w, 1e-10)\n",
    "fn = funObj(w)\n",
    "print(f)\n",
    "print(fn)\n",
    "print(gn)\n",
    "print(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
