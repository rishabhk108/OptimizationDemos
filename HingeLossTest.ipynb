{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Data\n",
    "\n",
    "We first generate a random dataset with number of features (m = 10) and number of instances (n = 100)\n",
    "We also generate a random label vector y \\in {-1,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "m = 10\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple naive Implementation of the Least Squares\n",
    "\n",
    "Below is a simple naive implementation of Least Square Loss. We directly plug in the formula with a simple for loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLossNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    f = 0\n",
    "    g = 0\n",
    "    for i in range(len(X)):\n",
    "        featureweightProd = np.dot(X[i],w)\n",
    "        f = f + np.max([0, 1 - y[i]*featureweightProd])\n",
    "        g = g - y[i]*X[i]*np.double(1 > y[i]*featureweightProd) \n",
    "    f = f + 0.5*lam*np.sum(w*w)\n",
    "    g = g + lam*w.reshape(1,-1)\n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "[f,g] = HingeLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Loop in Python == Slow Code\n",
    "\n",
    "Great, we have a working code now. But while this code might be correct, is this going to be fast? We have a For loop in python which is clearly an issue!\n",
    "\n",
    "First let us see how slow the code is! Let us increase n to 10000000 and m to 1000, which are somewhat more realistic (though still far from real world)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000\n",
    "m = 100\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = HingeLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the code!\n",
    "\n",
    "With n = 10000000, it takes around 4.5 minutes to run a single function evaluation!\n",
    "\n",
    "Lets now vectorize the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw = np.matmul(X,w)\n",
    "yT = y.reshape(-1,1)\n",
    "yXw = np.multiply(yT,Xw)\n",
    "np.shape(yXw)\n",
    "#f = np.sum(np.max(0, 1 - yXw.T)) + 0.5*np.sum(w*w)\n",
    "#print(f)\n",
    "#ymul = -1*yT*np.double(1 > yXw) \n",
    "#print(np.shape(ymul.reshape(1,-1)))\n",
    "#print(np.shape(X))\n",
    "#g = np.matmul(ymul.reshape(1,-1),X).reshape(-1,1)  + 1*w.reshape(-1,1)\n",
    "#print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLoss(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    Xw = np.matmul(X,w)\n",
    "    yT = y.reshape(-1,1)\n",
    "    yXw = np.multiply(yT,Xw)\n",
    "    f = np.sum(np.maximum(0, 1 - yXw.T)) + 0.5*np.sum(w*w)\n",
    "    ymul = -1*yT*np.double(1 > yXw)    \n",
    "    g = np.matmul(ymul.reshape(1,-1),X).reshape(-1,1)  + 1*w.reshape(-1,1)\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "[f,g] = HingeLoss(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking gradient implementations!\n",
    "\n",
    "So far so good! But how do we verify if our gradient implementation is correct?\n",
    "We can test out our loss function analytically, but what if we make a mistake in computing the gradient? We can numerically compute the gradient to ensure it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresFun(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    m = X.shape[0]\n",
    "    Xw = np.matmul(X,w)\n",
    "    Xwy = (Xw - y).reshape(-1,1)\n",
    "    f = np.dot(Xwy.T,Xwy) + 0.5*lam*np.sum(w*w)\n",
    "    return f\n",
    "\n",
    "def numericalGrad(funObj, w,epsilon):\n",
    "    m = len(w)\n",
    "    grad = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        wp = np.copy(w)\n",
    "        wn = np.copy(w)\n",
    "        wp[i] = w[i] + epsilon\n",
    "        wn[i] = w[i] - epsilon\n",
    "        grad[i] = (funObj(wp) - funObj(wn))/(2*epsilon)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "m = 10\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "wgen = np.random.rand(m)\n",
    "y = np.dot(X,wgen) + np.random.normal(0, 0.1, n)\n",
    "w = np.random.rand(m)\n",
    "\n",
    "funObj = lambda w: LeastSquaresFun(w,X,y,1)\n",
    "[f,g] = LeastSquares(w,X,y,1)\n",
    "gn = numericalGrad(funObj, w, 1e-10)\n",
    "fn = funObj(w)\n",
    "print(f)\n",
    "print(fn)\n",
    "print(gn)\n",
    "print(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
