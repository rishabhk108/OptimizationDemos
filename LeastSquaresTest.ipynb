{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Data\n",
    "\n",
    "We first generate a random dataset with number of features (m = 10) and number of instances (n = 100)\n",
    "We also generate a random label vector y \\in {-1,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "m = 10\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "wgen = np.random.rand(m)\n",
    "y = np.dot(X,wgen) + np.random.normal(0, 0.1, n)\n",
    "w = np.random.rand(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple naive Implementation of the Least Squares\n",
    "\n",
    "Below is a simple naive implementation of Least Square Loss. We directly plug in the formula with a simple for loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    f = 0\n",
    "    g = 0\n",
    "    for i in range(len(X)):\n",
    "        featureweightProd = np.dot(X[i],w)\n",
    "        f = f + (featureweightProd - y[i])*(featureweightProd - y[i])\n",
    "        g = g + 2*(featureweightProd - y[i])*X[i]\n",
    "    f = f + 0.5*lam*np.sum(w*w)\n",
    "    g = g + lam*w.reshape(1,-1)\n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.0012369155883789062\n",
      "Function value = 18.11430323106446\n",
      "Printing Gradient:\n",
      "[[-26.40587054 -10.08174756 -20.06086287 -26.54846283 -24.77238833\n",
      "  -19.75775787 -28.39635462 -27.87900945 -32.93241229 -30.65521977]]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = LeastSquaresNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Loop in Python == Slow Code\n",
    "\n",
    "Great, we have a working code now. But while this code might be correct, is this going to be fast? We have a For loop in python which is clearly an issue!\n",
    "\n",
    "First let us see how slow the code is! Let us increase n to 10000000 and m to 1000, which are somewhat more realistic (though still far from real world)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 3.6550180912017822\n",
      "Function value = 2979067.713990548\n"
     ]
    }
   ],
   "source": [
    "n = 1000000\n",
    "m = 100\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "wgen = np.random.rand(m)\n",
    "y = np.dot(X,wgen) + np.random.normal(0, 0.1, n)\n",
    "w = np.random.rand(m)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LeastSquaresNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the code!\n",
    "\n",
    "With n = 10000000, it takes around 2 minutes to run a single function evaluation!\n",
    "\n",
    "Lets now vectorize the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquares(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    m = X.shape[0]\n",
    "    Xw = np.matmul(X,w)\n",
    "    Xwy = (Xw - y).reshape(-1,1)\n",
    "    f = np.dot(Xwy.T,Xwy) + 0.5*lam*np.sum(w*w)\n",
    "    g = 2*np.dot(X.T,Xwy)  + lam*w.reshape(-1,1)\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.1029510498046875\n",
      "Function value = [[2979067.71399057]]\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = LeastSquares(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(np.shape(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking gradient implementations!\n",
    "\n",
    "So far so good! But how do we verify if our gradient implementation is correct?\n",
    "We can test out our loss function analytically, but what if we make a mistake in computing the gradient? We can numerically compute the gradient to ensure it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresFun(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    m = X.shape[0]\n",
    "    Xw = np.matmul(X,w)\n",
    "    Xwy = (Xw - y).reshape(-1,1)\n",
    "    f = np.dot(Xwy.T,Xwy) + 0.5*lam*np.sum(w*w)\n",
    "    return f\n",
    "\n",
    "def numericalGrad(funObj, w,epsilon):\n",
    "    m = len(w)\n",
    "    grad = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        wp = np.copy(w)\n",
    "        wn = np.copy(w)\n",
    "        wp[i] = w[i] + epsilon\n",
    "        wn[i] = w[i] - epsilon\n",
    "        grad[i] = (funObj(wp) - funObj(wn))/(2*epsilon)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29.21435641]]\n",
      "[[29.21435641]]\n",
      "[32.52422331 29.83098213 40.37449486 45.67565881 30.93648004 52.74085169\n",
      " 38.37445917 41.6134327  38.7920096  46.16868665]\n",
      "[[32.52420457]\n",
      " [29.8309785 ]\n",
      " [40.37451531]\n",
      " [45.67562758]\n",
      " [30.93648274]\n",
      " [52.74079572]\n",
      " [38.3744738 ]\n",
      " [41.61338888]\n",
      " [38.79201958]\n",
      " [46.16868019]]\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "wgen = np.random.rand(m)\n",
    "y = np.dot(X,wgen) + np.random.normal(0, 0.1, n)\n",
    "w = np.random.rand(m)\n",
    "\n",
    "funObj = lambda w: LeastSquaresFun(w,X,y,1)\n",
    "[f,g] = LeastSquares(w,X,y,1)\n",
    "gn = numericalGrad(funObj, w, 1e-10)\n",
    "fn = funObj(w)\n",
    "print(f)\n",
    "print(fn)\n",
    "print(gn)\n",
    "print(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
