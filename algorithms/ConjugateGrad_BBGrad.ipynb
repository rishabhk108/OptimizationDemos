{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Lets Load the dataset. We shall use the following datasets:\n",
    "Features are in: \"sido0_train.mat\"\n",
    "Labels are in: \"sido0_train.targets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12678, 4932)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "X = loadmat(r\"/Users/rkiyer/Desktop/teaching/CS6301/jupyter/data/sido0_matlab/sido0_train.mat\")\n",
    "y = np.loadtxt(r\"/Users/rkiyer/Desktop/teaching/CS6301/jupyter/data/sido0_matlab/sido0_train.targets\")\n",
    "\n",
    "# Statistics of the Dense Format of X\n",
    "X = X['X'].todense()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Definition\n",
    "Lets use the Logistic Regression definition we previously used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLoss(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    m = X.shape[0]\n",
    "    Xw = np.dot(X,w)\n",
    "    yT = y.reshape(-1,1)\n",
    "    yXw = np.multiply(yT,Xw)\n",
    "    f = np.sum(np.logaddexp(0,-yXw)) + 0.5*lam*np.sum(np.multiply(w,w))\n",
    "    gMul = 1/(1 + np.exp(yXw))\n",
    "    ymul = -1*np.multiply(yT, gMul)\n",
    "    g =  np.dot(ymul.reshape(1,-1),X) + lam*w.reshape(1,-1)\n",
    "    g = g.reshape(-1,1)\n",
    "    return [f, g]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barzelia Borwein step length\n",
    "Lets invoke BB Step Length Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def gdBB(funObj,w,maxEvals,alpha,gamma,X,y,lam, verbosity, freq):\n",
    "    [f,g] = funObj(w,X,y,lam)\n",
    "    funEvals = 1\n",
    "    funVals = []\n",
    "    f_old = f\n",
    "    g_old = g\n",
    "    funVals.append(f)\n",
    "    numBackTrack = 0\n",
    "    while(1):\n",
    "        wp = w - alpha*g\n",
    "        [fp,gp] = funObj(wp,X,y,lam)\n",
    "        funVals.append(f)\n",
    "        funEvals = funEvals+1\n",
    "        backtrack = 0\n",
    "        if funEvals > 2:\n",
    "            g_diff = g - g_old\n",
    "            alpha = -alpha*np.dot(g_old.T, g_diff)[0,0]/np.dot(g_diff.T, g_diff)[0,0]\n",
    "        while fp > f - gamma*alpha*np.dot(g.T, g):\n",
    "            alpha = alpha*alpha*np.dot(g.T, g)[0,0]/(2*(fp + np.dot(g.T, g)[0,0]*alpha - f))\n",
    "            wp = w - alpha*g\n",
    "            [fp,gp] = funObj(wp,X,y,lam)\n",
    "            funVals.append(f)\n",
    "            funEvals = funEvals+1\n",
    "            numBackTrack = numBackTrack + 1\n",
    "        f_old = f\n",
    "        g_old = g\n",
    "        w = wp\n",
    "        f = fp\n",
    "        g = gp\n",
    "        optCond = LA.norm(g, np.inf)\n",
    "        if ((verbosity > 0) and (funEvals % freq == 0)):\n",
    "            print(funEvals,alpha,f,optCond)\n",
    "        if (optCond < 1e-2):\n",
    "            break\n",
    "        if (funEvals >= maxEvals):\n",
    "            break\n",
    "    return (funVals,numBackTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 2.5788899258035455e-05 1135.7630614160032 76.76587360949645\n",
      "40 3.4479636532951836e-05 932.6568131903588 87.65958408181322\n",
      "50 4.021247435512306e-06 849.9100523680455 43.4455639307481\n",
      "70 1.7793550062790673e-05 776.6339625092082 56.759560073959456\n",
      "80 0.00013351557593887492 744.1113239997849 39.28022856286154\n",
      "100 8.059370484177815e-09 687.6850460523219 74.14968315884308\n",
      "110 0.0003390191457756867 661.652761006767 21.602772936085945\n",
      "120 0.004059006282489887 518.5101896471618 43.03303982013437\n",
      "130 3.537647992869348e-05 513.0734220349092 8.84234859224609\n",
      "150 0.00254417659060005 448.2235645802006 80.78196339937944\n",
      "160 7.613967174596072e-05 424.1394663061126 16.374169898800403\n",
      "170 2.5668319295286767e-08 416.9102811833201 49.00940438219084\n",
      "190 1.1284701287159109e-05 379.81228121212956 21.89514315379142\n",
      "220 9.464450390497666e-07 237.86073095644207 9.047437842072526\n",
      "230 0.004223306733352749 227.94326633703025 50.92814591412953\n",
      "240 1.60654955766724e-05 216.1835035208207 3.451857941580089\n",
      "250 4.134166791547012e-05 153.586334083372 7.265833688540766\n",
      "250\n",
      "Number of Backtrackings = 75\n"
     ]
    }
   ],
   "source": [
    "[nSamples,nVars] = X.shape\n",
    "w = np.zeros((nVars,1))\n",
    "(funV1,numBackTrack) = gdBB(LogisticLoss,w,250,1,1e-4,X,y,1,1,10)\n",
    "print(len(funV1))\n",
    "print(\"Number of Backtrackings = \" + str(numBackTrack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradient Descent\n",
    "Nonlinear Conjugate Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def gdCG(funObj,w,maxEvals,alpha,gamma,X,y,lam, verbosity, freq):\n",
    "    [f,g] = funObj(w,X,y,lam)\n",
    "    funEvals = 1\n",
    "    funVals = []\n",
    "    f_old = f\n",
    "    g_old = g\n",
    "    funVals.append(f)\n",
    "    numBackTrack = 0\n",
    "    d = g\n",
    "    while(1):\n",
    "        wp = w - alpha*d\n",
    "        [fp,gp] = funObj(wp,X,y,lam)\n",
    "        funVals.append(f)\n",
    "        funEvals = funEvals+1\n",
    "        backtrack = 0\n",
    "        if funEvals > 2:\n",
    "            alpha = min(1,2*(f_old - f)/np.dot(g.T, g)[0,0])\n",
    "            beta = np.dot(g.T, g)[0,0]/np.dot(g_old.T, g_old)[0,0]\n",
    "            d = g + beta*d\n",
    "        else:\n",
    "            d = g\n",
    "        while fp > f - gamma*alpha*np.dot(g.T, d)[0,0]:\n",
    "            alpha = alpha*alpha*np.dot(g.T, d)[0,0]/(2*(fp + np.dot(g.T, d)[0,0]*alpha - f))\n",
    "            wp = w - alpha*d\n",
    "            [fp,gp] = funObj(wp,X,y,lam)\n",
    "            funVals.append(f)\n",
    "            funEvals = funEvals+1\n",
    "            numBackTrack = numBackTrack + 1\n",
    "        f_old = f\n",
    "        g_old = g\n",
    "        w = wp\n",
    "        f = fp\n",
    "        g = gp\n",
    "        optCond = LA.norm(g, np.inf)\n",
    "        if ((verbosity > 0) and (funEvals % freq == 0)):\n",
    "            print(funEvals,alpha,f,optCond)\n",
    "        if (optCond < 1e-2):\n",
    "            break\n",
    "        if (funEvals >= maxEvals):\n",
    "            break\n",
    "    return (funVals,numBackTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 4.8248838390056086e-05 1100.5825812169905 71.17864014090668\n",
      "40 5.761205021221726e-06 1020.2047566018817 60.08817550984056\n",
      "50 7.00499199239626e-07 1002.4726260840971 153.62023184767298\n",
      "60 7.545618878621844e-06 925.5611967492454 85.61509239991781\n",
      "70 1.1115290013781674e-07 835.8507464241959 91.58198734211311\n",
      "90 4.441863657411808e-05 751.702281580756 101.83346471201462\n",
      "100 4.1046760588252236e-06 732.1216358152724 86.17994084529874\n",
      "110 2.3993096858746483e-05 644.2756391402414 27.928830911694643\n",
      "120 1.0056307504898691e-06 619.8412799763845 57.64301891477717\n",
      "130 3.1198498348610805e-06 611.8077214404658 49.09555617390839\n",
      "140 1.1188637267886875e-06 543.3696061152098 85.6780959606216\n",
      "150 1.9476528884453286e-06 521.0911809959176 57.99381049868182\n",
      "160 3.809031251617791e-06 436.93965622540577 60.16531562869829\n",
      "170 8.993239281748671e-06 432.4501776394318 36.25034076350421\n",
      "190 9.097237262686172e-06 390.3268532288466 40.463044973828325\n",
      "200 2.959477446275187e-07 388.0645119908878 34.67348963196586\n",
      "210 4.0064325422324243e-08 323.9043367568432 104.47445243511075\n",
      "230 4.2860917660356265e-06 302.5701932483058 29.175853310706646\n",
      "240 3.2014786140553776e-06 300.7658397250583 38.647272965098615\n",
      "250 9.915357890415802e-06 287.6046370596453 55.160188266289865\n",
      "250\n",
      "Number of Backtrackings = 73\n"
     ]
    }
   ],
   "source": [
    "[nSamples,nVars] = X.shape\n",
    "w = np.zeros((nVars,1))\n",
    "(funV1,numBackTrack) = gdCG(LogisticLoss,w,250,1,1e-4,X,y,1,1,10)\n",
    "print(len(funV1))\n",
    "print(\"Number of Backtrackings = \" + str(numBackTrack))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
